{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "from shapely.geometry import Polygon, box\n",
    "from shapely.affinity import scale, affine_transform\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions for feature exctraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot image with given polygons\n",
    "def plot_picture_with_polygons(image, polygons):\n",
    "    fig, ax = plt.subplots(1, figsize=(20,12))\n",
    "    ax.imshow(image)\n",
    "    for polygon in polygons:\n",
    "        points = polygon\n",
    "        polygon = patches.Polygon(points, closed=True, edgecolor='red', fill=False, linewidth=2, label=label)\n",
    "        ax.add_patch(polygon)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find rough areas in picture\n",
    "def rough_image_filter(image, threshold=8):\n",
    "\n",
    "    #read in image as greyscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #apply local binary pattern\n",
    "    lbp = local_binary_pattern(image, P=8, R=2, method='uniform')\n",
    "\n",
    "    lbp_uint8 = np.uint8((lbp / lbp.max())*255)\n",
    "    \n",
    "    blurred = cv2.blur(lbp_uint8, (5, 5), 0)\n",
    "    mean = np.mean(blurred)\n",
    "    std = np.std(blurred)\n",
    "    median = np.median(blurred)\n",
    "    #apply thresholding\n",
    "    _, binary_image = cv2.threshold(blurred, mean - std, 255, cv2.THRESH_BINARY) \n",
    "\n",
    "    binary_blurred = cv2.blur(binary_image, (50, 50), 0)\n",
    "    mean = np.mean(binary_blurred)\n",
    "    std = np.std(binary_blurred)\n",
    "    #apply thresholding\n",
    "    _, binary_image = cv2.threshold(binary_blurred, mean - 1.5 * std, 255, cv2.THRESH_BINARY) # normlaize and convert to uint8, then blur\n",
    "\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to identify reddisch areas in an image\n",
    "def reddish_image_filter(image):\n",
    "\n",
    "    def color_detector(image, lower_bounds: list, upper_bounds: list) -> list:\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        mask = np.zeros(hsv_image.shape[:2], dtype=np.uint8)\n",
    "        for lower_bound, upper_bound in zip(lower_bounds, upper_bounds):\n",
    "            lower_bound = np.array(lower_bound)\n",
    "            upper_bound = np.array(upper_bound)\n",
    "            mask += cv2.inRange(hsv_image, lower_bound, upper_bound)\n",
    "        return mask\n",
    "\n",
    "    rusty_lower_bounds = [[0, 40, 50]]\n",
    "    rusty_upper_bounds = [[20, 255, 200]]\n",
    "    reddish_areas = color_detector(image, rusty_lower_bounds, rusty_upper_bounds)\n",
    "\n",
    "    return reddish_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find darker areas in an image\n",
    "def darker_image_filter(image, z = 200):\n",
    "    # Convert image to grayscale:\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    grey_image = cv2.blur(grey_image, (15, 15), 0)\n",
    "    mean = np.mean(grey_image)\n",
    "    std = np.std(grey_image) / 250\n",
    "    dark_areas = (grey_image < mean - z * std)\n",
    "    dark_areas = dark_areas.astype(np.uint8)\n",
    "    blurred = cv2.blur(dark_areas, (5, 5), 0) *255\n",
    "    return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#legthy image filter\n",
    "def lengthy_image_filter(image, ratio = 2.5):\n",
    "# Blur image:\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = cv2.GaussianBlur(gray_image, (7, 7), 0)\n",
    "\n",
    "    # Apply edge detector:\n",
    "    edges = cv2.Canny(gray_image, 50, 150)\n",
    "\n",
    "    # Apply morphological operations \n",
    "    kernel = np.ones((15, 15), np.uint8)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    edges = cv2.erode(edges, kernel, iterations=1)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    stencil = np.zeros(gray_image.shape).astype(gray_image.dtype)\n",
    "    color = [255]\n",
    "    for contour in contours:\n",
    "        try:\n",
    "            # Get fitted bounding box\n",
    "            rect = cv2.minAreaRect(contour)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            width = rect[1][0]\n",
    "            height = rect[1][1]\n",
    "            \n",
    "            # Calculate aspect ratio\n",
    "            aspect_ratio = float(max(width, height)) / min(width, height)\n",
    "            if aspect_ratio > 2.5:\n",
    "                cv2.drawContours(stencil, [box], 0, (0, 0, 255), 2)\n",
    "                cv2.fillPoly(stencil, [box], color)\n",
    "        except:\n",
    "            pass\n",
    "    return stencil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get an image with edges for the lengthy objects filter:\n",
    "def lengthy_image_filter_edges(image):\n",
    "    \n",
    "    # Read in image as greyscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Blur image:\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (7, 7), 0)\n",
    "\n",
    "    # Apply edge detector:\n",
    "    edges = cv2.Canny(blurred_image, 50, 150)\n",
    "\n",
    "    # Apply morphological operations \n",
    "    kernel = np.ones((15, 15), np.uint8)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    edges = cv2.erode(edges, kernel, iterations=1)\n",
    "\n",
    "    return edges\n",
    "\n",
    "# Function to get number of contours with aspect ratio > 2:\n",
    "def extract_lengthy_features_1(edges, defect_polygon):\n",
    "\n",
    "    # Create defect polygon and calculate its width and height:\n",
    "    defect_polygon = Polygon(defect_polygon)\n",
    "    defect_rect_coords = np.array(list(defect_polygon.minimum_rotated_rectangle.exterior.coords)[:-1])\n",
    "    distances = [np.linalg.norm(defect_rect_coords[i] - defect_rect_coords[(i + 1) % len(defect_rect_coords)]) for i in range(len(defect_rect_coords))]\n",
    "    defect_width, defect_height = sorted(distances)[0], sorted(distances)[-1]\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Extract overlapping contours (where >80% of the contour area is overlapping with the polygon):\n",
    "    overlapping_contours = []\n",
    "    for contour in contours: \n",
    "        if not np.array_equal(contour[0], contour[-1]):\n",
    "            contour = np.vstack([contour, contour[0:1]])\n",
    "        contour_as_polygon = Polygon([(x, y) for x, y in contour[:, 0]])\n",
    "        try:\n",
    "            intersection = contour_as_polygon.intersection(defect_polygon)\n",
    "            intersection_area = intersection.area\n",
    "        except:\n",
    "            intersection_area = 0\n",
    "        contour_area = contour_as_polygon.area\n",
    "        if intersection_area > 0 and intersection_area >= 0.8 * contour_area:\n",
    "            overlapping_contours.append(contour)\n",
    "\n",
    "    # Extract characteristics per overlapping contour:\n",
    "    characteristics = []\n",
    "    lengths = []\n",
    "    for contour in overlapping_contours:\n",
    "\n",
    "        # Get fitted bounding box\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "        width = max(rect[1][0], 1)\n",
    "        height = max(rect[1][1], 1)\n",
    "        \n",
    "        # Calculate aspect ratio\n",
    "        aspect_ratio = float(max(width, height) / min(width, height))\n",
    "        characteristics.append(aspect_ratio)\n",
    "        lengths.append(max(width, height))\n",
    "    \n",
    "    # Extract number of contours with aspect ratio >= 2 and the average aspect ratio of these contours:\n",
    "    number = 0\n",
    "    avg_aspect_ratio_lengthy = 0\n",
    "    for aspect_ratio in characteristics:\n",
    "        if aspect_ratio >= 2.5:\n",
    "            avg_aspect_ratio_lengthy += aspect_ratio\n",
    "            number += 1\n",
    "    avg_aspect_ratio_lengthy = max(avg_aspect_ratio_lengthy / max(number, 1), 1)\n",
    "\n",
    "    # Extract the quotient (length of the lengthiest contour) / (length of the defect):\n",
    "    rel_length = (max(lengths) / max(defect_width, defect_height)) if len(lengths) > 0 else 0\n",
    "\n",
    "    return number, avg_aspect_ratio_lengthy, rel_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to evaluate / visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlapping_values(filtered_image, defect_polygon):\n",
    "    # Generate polygon mask:\n",
    "    mask = np.zeros(filtered_image.shape[:2], dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [defect_polygon], 255)\n",
    "\n",
    "    # Extract overlapping pixel values:\n",
    "    overlapping_values = filtered_image[mask == 255]\n",
    "    overlapping_values = overlapping_values.tolist()\n",
    "    \n",
    "    return overlapping_values\n",
    "\n",
    "def get_relative_frequencies(values: list) -> dict:\n",
    "    \n",
    "    counts = Counter(values)\n",
    "    total_count = len(values)\n",
    "    relative_frequencies = {element: count / total_count for element, count in counts.items()}\n",
    "    return relative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddish_feature(image, defect_polygon):\n",
    "\n",
    "    overlapping_values = get_overlapping_values(image, defect_polygon)\n",
    "    relative_frequencies = get_relative_frequencies(overlapping_values)\n",
    "    try:\n",
    "        quotient = relative_frequencies[255]\n",
    "    except:\n",
    "        quotient = 0\n",
    "    return quotient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rough_feature(image, defect_polygon):\n",
    "\n",
    "    overlapping_values = get_overlapping_values(image, defect_polygon)\n",
    "    relative_frequencies = get_relative_frequencies(overlapping_values)\n",
    "    try:\n",
    "        quotient = relative_frequencies[0]\n",
    "    except:\n",
    "        quotient = 0\n",
    "    return quotient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dark_feature(image, defect_polygon):\n",
    "\n",
    "    overlapping_values = get_overlapping_values(image, defect_polygon)\n",
    "    relative_frequencies = get_relative_frequencies(overlapping_values)\n",
    "    try:\n",
    "        quotient = relative_frequencies[255]\n",
    "    except:\n",
    "        quotient = 0\n",
    "    return quotient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lengthy_feature(image, defect_polygon):\n",
    "    overlapping_values = get_overlapping_values(image, defect_polygon)\n",
    "    relative_frequencies = get_relative_frequencies(overlapping_values)\n",
    "    try:\n",
    "        quotient = relative_frequencies[255]\n",
    "    except:\n",
    "        quotient = 0\n",
    "    return quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koeni\\AppData\\Local\\Temp\\ipykernel_57788\\1411049059.py:25: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  box = np.int0(box)\n",
      "C:\\Users\\koeni\\AppData\\Local\\Temp\\ipykernel_57788\\354445803.py:55: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  box = np.int0(box)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 0010\n",
      "\n",
      "Processing image 0020\n",
      "\n",
      "Processing image 0030\n",
      "\n",
      "Processing image 0040\n"
     ]
    }
   ],
   "source": [
    "n_pictures = 50\n",
    "samples = []\n",
    "no_defects = 0\n",
    "\n",
    "for i in range(n_pictures):\n",
    "    i = str(i).zfill(4)\n",
    "    if int(i) % 10 == 0:\n",
    "        print()\n",
    "        print(f\"Processing image {i}\")\n",
    "\n",
    "    #Bild einlesen\n",
    "    image_path = f\"data/dacl10k_v2_devphase/images/train/dacl10k_v2_train_{i}.jpg\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    #Defekte einlesen\n",
    "    annotations_path = f\"data/dacl10k_v2_devphase/annotations/train/dacl10k_v2_train_{i}.json\"\n",
    "    with open(annotations_path, 'r') as file:\n",
    "        annotations = json.load(file)\n",
    "    defects = annotations['shapes']\n",
    "\n",
    "    #Areas finden\n",
    "    #print(\"Finding darker areas\")\n",
    "    darker_image = darker_image_filter(image)\n",
    "    #print(\"Finding reddish areas\")\n",
    "    reddish_image = reddish_image_filter(image)\n",
    "    #print(\"Finding rough areas\")\n",
    "    rough_image = rough_image_filter(image)\n",
    "    #print(\"Finding lengthy areas\")\n",
    "    lengthy_image = lengthy_image_filter(image)\n",
    "    edges_for_lengthy_feature = lengthy_image_filter_edges(image)\n",
    "\n",
    "    for k in range(len(defects)):\n",
    "        label = defects[k]['label']\n",
    "        if label in [\"Rust\", \"Graffiti\", \"Drainage\", \"Wetspot\", \"ExposedRebars\", \"Crack\"]:\n",
    "            no_defects += 1\n",
    "            #print(\"defectpolygon: \", no_defects)\n",
    "            #plot_picture_with_polygons(image, rough_polygons)\n",
    "            defect_polygon = np.array(defects[k]['points'], dtype = np.int32)\n",
    "            #print(\"Calculating darker quotient\")\n",
    "            darker_quotient = extract_dark_feature(darker_image, defect_polygon)\n",
    "            #print(\"Calculating reddish quotient\")\n",
    "            reddish_quotient = extract_reddish_feature(reddish_image, defect_polygon)\n",
    "            #print(\"Calculating rough quotient\")\n",
    "            rough_quotient = extract_rough_feature(rough_image, defect_polygon)\n",
    "            #print(\"Calculating lengthy quotient\")\n",
    "            lengthy_quotient = extract_lengthy_feature(lengthy_image, defect_polygon)\n",
    "            number_lengthy_objects, avg_aspect_ratio, rel_length = extract_lengthy_features_1(edges_for_lengthy_feature, defect_polygon)\n",
    "\n",
    "            temp_dict = {'label': label, 'darker': darker_quotient, 'reddish': reddish_quotient, 'rough': rough_quotient, 'lengthy': lengthy_quotient, \n",
    "                         'lengthy_1': number_lengthy_objects, 'lengthy_2': avg_aspect_ratio, 'lengthy_3': rel_length}\n",
    "            samples.append(temp_dict)\n",
    "\n",
    "samples = pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafitti:\n",
      "darker       0.333082\n",
      "reddish      0.142308\n",
      "rough        0.168847\n",
      "lengthy      0.276332\n",
      "lengthy_1    1.826389\n",
      "lengthy_2    2.440175\n",
      "lengthy_3    0.221588\n",
      "dtype: float64\n",
      "\n",
      "Rust:\n",
      "darker       0.279984\n",
      "reddish      0.459263\n",
      "rough        0.056051\n",
      "lengthy      0.316078\n",
      "lengthy_1    0.195652\n",
      "lengthy_2    1.409282\n",
      "lengthy_3    0.116220\n",
      "dtype: float64\n",
      "\n",
      "Drainage:\n",
      "darker       0.530238\n",
      "reddish      0.066249\n",
      "rough        0.123530\n",
      "lengthy      0.465275\n",
      "lengthy_1    0.200000\n",
      "lengthy_2    1.323928\n",
      "lengthy_3    0.096764\n",
      "dtype: float64\n",
      "\n",
      "Wetspot:\n",
      "darker       0.820379\n",
      "reddish      0.089808\n",
      "rough        0.047576\n",
      "lengthy      0.255191\n",
      "lengthy_1    1.090909\n",
      "lengthy_2    3.933449\n",
      "lengthy_3    0.117216\n",
      "dtype: float64\n",
      "\n",
      "ExposedRebars:\n",
      "darker       0.402304\n",
      "reddish      0.427987\n",
      "rough        0.136835\n",
      "lengthy      0.235946\n",
      "lengthy_1    0.285714\n",
      "lengthy_2    1.342857\n",
      "lengthy_3    0.361696\n",
      "dtype: float64\n",
      "\n",
      "Crack:\n",
      "darker       0.158233\n",
      "reddish      0.310179\n",
      "rough        0.087749\n",
      "lengthy      0.400276\n",
      "lengthy_1    0.222222\n",
      "lengthy_2    6.432609\n",
      "lengthy_3    0.105550\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "graffiti = samples[samples['label'] == 'Graffiti']\n",
    "rust = samples[samples['label'] == 'Rust']\n",
    "drainage = samples[samples['label'] == 'Drainage']\n",
    "wetspot = samples[samples['label'] == 'Wetspot']\n",
    "exposedrebars = samples[samples['label'] == 'ExposedRebars']\n",
    "crack = samples[samples['label'] == 'Crack']\n",
    "\n",
    "print(\"Grafitti:\")\n",
    "print(graffiti.loc[:,[\"darker\", \"reddish\", \"rough\", \"lengthy\", \"lengthy_1\", \"lengthy_2\", \"lengthy_3\"]].mean(axis = 0))\n",
    "print()\n",
    "\n",
    "print(\"Rust:\")\n",
    "print(rust.loc[:,[\"darker\", \"reddish\", \"rough\", \"lengthy\", \"lengthy_1\", \"lengthy_2\", \"lengthy_3\"]].mean(axis = 0))\n",
    "print()\n",
    "\n",
    "print(\"Drainage:\")\n",
    "print(drainage.loc[:,[\"darker\", \"reddish\", \"rough\", \"lengthy\", \"lengthy_1\", \"lengthy_2\", \"lengthy_3\"]].mean(axis = 0))\n",
    "print()\n",
    "\n",
    "print(\"Wetspot:\")\n",
    "print(wetspot.loc[:,[\"darker\", \"reddish\", \"rough\", \"lengthy\", \"lengthy_1\", \"lengthy_2\", \"lengthy_3\"]].mean(axis = 0))\n",
    "print()\n",
    "\n",
    "print(\"ExposedRebars:\")\n",
    "print(exposedrebars.loc[:,[\"darker\", \"reddish\", \"rough\", \"lengthy\", \"lengthy_1\", \"lengthy_2\", \"lengthy_3\"]].mean(axis = 0))\n",
    "print()\n",
    "\n",
    "print(\"Crack:\")\n",
    "print(crack.loc[:,[\"darker\", \"reddish\", \"rough\", \"lengthy\", \"lengthy_1\", \"lengthy_2\", \"lengthy_3\"]].mean(axis = 0))\n",
    "\n",
    "#samples.to_csv('samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_data(samples, target_label, features):\n",
    "\n",
    "    # Create rust labels:\n",
    "    target_class_df = samples.copy()\n",
    "    target_class_df[\"target_label\"] = target_class_df[\"label\"].apply(lambda x: 1 if x == target_label else 0)\n",
    "\n",
    "    # Apply train test split:\n",
    "    x = target_class_df.loc[:, features]\n",
    "    y = target_class_df.loc[:, \"target_label\"]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def run_classifier(samples, target_label, features):\n",
    "\n",
    "    # Generate train test dataset: \n",
    "    x_train, x_test, y_train, y_test = create_train_test_data(samples, target_label, features)\n",
    "\n",
    "    # Initialize the random forest classifier: \n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the classifier:\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Test the classifier: \n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Evaluate the classification:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Classification Report:\\n{report}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply classification for different defect classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rust classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6937062937062937\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.56      0.60       296\n",
      "           1       0.72      0.79      0.75       419\n",
      "\n",
      "    accuracy                           0.69       715\n",
      "   macro avg       0.68      0.67      0.68       715\n",
      "weighted avg       0.69      0.69      0.69       715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run classifier for rust: \n",
    "run_classifier(samples, \"Rust\", [\"reddish\", \"rough\", \"darker\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wetspot classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8909090909090909\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       672\n",
      "           1       0.11      0.12      0.11        43\n",
      "\n",
      "    accuracy                           0.89       715\n",
      "   macro avg       0.53      0.53      0.53       715\n",
      "weighted avg       0.89      0.89      0.89       715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rund classifier for wetspots:\n",
    "run_classifier(samples, \"Wetspot\", [\"darker\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graffiti classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6937062937062937\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.56      0.60       296\n",
      "           1       0.72      0.79      0.75       419\n",
      "\n",
      "    accuracy                           0.69       715\n",
      "   macro avg       0.68      0.67      0.68       715\n",
      "weighted avg       0.69      0.69      0.69       715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run classifier for Graffiti:\n",
    "run_classifier(samples, \"Rust\", [\"reddish\", \"rough\", \"darker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "# implement blue, yellow, green and shiny filter -> Samuel\n",
    "# implement colourfullness measure -> Samuel\n",
    "# implement circle detector -> Samuel\n",
    "# in rough filter take lbp values in polygon as feature -> Samuel\n",
    "# rough filter take entropy as feature -> Samuel\n",
    "# darkness gradient for wetspots -> Felix\n",
    "# test hough lines detector for lengthy feature -> Felix\n",
    "# Straightness feature -> Felix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function dump / not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the overlapping percentage of a defect polygon with feature polygons\n",
    "def calculate_overlapping_percentage(defect_polygon, feature_polygons):\n",
    "    overlapping_quotient = []\n",
    "    defect_polygon = Polygon(defect_polygon)\n",
    "    overlapping_area = 0\n",
    "    for feature_polygon in feature_polygons:\n",
    "        if len(feature_polygon) > 3:\n",
    "            feature_polygon = Polygon(feature_polygon)\n",
    "            try:\n",
    "                overlapping_area += defect_polygon.intersection(feature_polygon).area\n",
    "            except:\n",
    "                pass\n",
    "    quotient = overlapping_area / defect_polygon.area\n",
    "    return round(quotient*100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find darker areas in an image\n",
    "def find_darker_areas(image, threshold=100):\n",
    "    # Convert image to grayscale:\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Find dark areas:\n",
    "    dark_areas = grey_image < threshold\n",
    "    dark_areas = dark_areas.astype(np.uint8)\n",
    "    blurred = cv2.blur(dark_areas, (15, 15), 0)\n",
    "    #plt.imshow(blurred)\n",
    "    contours, _ = cv2.findContours(blurred, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        points = []\n",
    "        for point in contour:\n",
    "            points.append(*point.tolist())\n",
    "        polygons.append(points)\n",
    "\n",
    "    return blurred, polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obsolete function to find reddish areas in an image\n",
    "def find_reddish_areas(image):\n",
    "\n",
    "    def color_detector(image, lower_bounds: list, upper_bounds: list) -> list:\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        mask = np.zeros(hsv_image.shape[:2], dtype=np.uint8)\n",
    "        for lower_bound, upper_bound in zip(lower_bounds, upper_bounds):\n",
    "            lower_bound = np.array(lower_bound)\n",
    "            upper_bound = np.array(upper_bound)\n",
    "            mask += cv2.inRange(hsv_image, lower_bound, upper_bound)\n",
    "        return mask\n",
    "\n",
    "    rusty_lower_bounds = [[0, 40, 50], [170, 40, 50]]\n",
    "    rusty_upper_bounds = [[10, 255, 200], [180, 255, 200]]\n",
    "    blurred = cv2.blur(image, (30, 30), 0)\n",
    "    reddish_areas = color_detector(blurred, rusty_lower_bounds, rusty_upper_bounds)\n",
    "    plt.imshow(reddish_areas)\n",
    "    contours, _ = cv2.findContours(reddish_areas, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        points = []\n",
    "        for point in contour:\n",
    "            points.append(*point.tolist())\n",
    "        polygons.append(points)\n",
    "\n",
    "    return image, polygons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rough_areas(image, threshold=8):\n",
    "\n",
    "    #read in image as greyscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #apply local binary pattern\n",
    "    lbp = local_binary_pattern(image, P=8, R=2, method='uniform')\n",
    "\n",
    "    lbp_uint8 = np.uint8((lbp / lbp.max())*255)\n",
    "    \n",
    "    blurred = cv2.blur(lbp_uint8, (5, 5), 0)\n",
    "    mean = np.mean(blurred)\n",
    "    std = np.std(blurred)\n",
    "    median = np.median(blurred)\n",
    "    #apply thresholding\n",
    "    _, binary_image = cv2.threshold(blurred, mean - std, 255, cv2.THRESH_BINARY) \n",
    "\n",
    "    binary_blurred = cv2.blur(binary_image, (50, 50), 0)\n",
    "    mean = np.mean(binary_blurred)\n",
    "    std = np.std(binary_blurred)\n",
    "    #apply thresholding\n",
    "    _, binary_image = cv2.threshold(binary_blurred, mean - 1.5 * std, 255, cv2.THRESH_BINARY) # normlaize and convert to uint8, then blur\n",
    "    plt.imshow(binary_image)\n",
    "    print(binary_image)\n",
    "    np.histogram(binary_image, bins=10, range=None, density=None, weights=None)\n",
    "    \n",
    "    #plt.imshow(binary_image)\n",
    "    \n",
    "    # Uncomment and adjust the contour finding and polygon processing as needed\n",
    "    \n",
    "    contours_outer, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #contours_tree, _ = cv2.findContours(binary_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #contours_ccomp, _ = cv2.findContours(binary_image, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    polygons = []\n",
    "    for contour in contours[0:-1]:\n",
    "        points = []\n",
    "        for point in contour:\n",
    "            points.append(*point.tolist())\n",
    "        polygons.append(points)\n",
    "\n",
    "    return lbp, polygons\n",
    "\n",
    "image_path = f\"data/dacl10k_v2_devphase/images/train/dacl10k_v2_train_0001.jpg\"\n",
    "image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "dark_areas, dark_polygons = find_rough_areas(image)\n",
    "#plot_picture_with_polygons(image, dark_polygons)\n",
    "print(len(dark_polygons))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
